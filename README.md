# Objectif

Cr√©ation d'une application web permettant de pr√©dire le prix d'un bien immobilier √† partir de certaines caract√©ristiques. L'objectif de ce TP est de vous familiariser avec les probl√©matiques de d√©ploiement d'un mod√®le de Machine Learning. Ce TP se d√©roule en 2 parties:

1. Cr√©ation d'un mod√®le de Machine Learning permettant de pr√©dire le prix d'un bien immobilier
2. Cr√©ation d'une webapp permettant de faire appel √† ce mod√®le

# Ressources

Pour r√©aliser ce TP vous aurez besoin d'avoir [Python](https://www.python.org/downloads/) install√© sur votre ordinateur, ainsi que de pouvoir ouvrir et ex√©cuter un Jupyter notebook. Nous vous conseillons d'installer [VsCode](https://code.visualstudio.com/), un √©diteur de code d√©velopp√© par Microsoft, afin de faciliter ce travail.

De plus vous aurez besoin d'√™tre familier avec les √©l√©ments suivants:

- [`Scikit-Learn`](https://scikit-learn.org/stable/) qui est une librairie Python permettant de facilement utiliser des mod√®les de Machine Learning.
- [`Pickle`](https://docs.python.org/3/library/pickle.html) permet de s√©rialiser des objets Python (transformation d'une structure Python en un fichier pouvant √™tre stock√© puis reconstruit).
- [`Pandas`](https://pandas.pydata.org/) une librairie permettant de manipuler des DataFrame tr√®s facilement.
- [`Flask`](https://flask.palletsprojects.com/en/2.2.x/) est un framework de d√©veloppement d'appication Web id√©al pour cr√©er des APIs.

# Installation

**Toutes les commandes suivantes sont √† ex√©cuter dans un terminal qui peut √™tre lanc√© sur VSCode**

1. Vous pouvez cloner ce repository en utilisant la commande:

```bash
git clone git@github.com:fapont/imt-exercice.git
```

Cela va cr√©er un dossier `imt-exercice` contenant tous les fichiers n√©cessaires pour ce TP.

2. Si Python n'est pas install√© sur votre ordinateur, vous pouvez le faire en suivant [ce tutoriel](https://realpython.com/installing-python/).
   Sur les machines utilis√©es √† l'IMT, Python est d√©j√† install√©. Vous pouvez v√©rifier que Python est bien install√© en utilisant la commande:

```bash
python --version
ou
python3 --version
```

3. Installer les d√©pendances n√©cessaires pour ce TP.

```bash
cd imt-exercice # Se placer dans le dossier du TP
pip install -r requirements.txt
ou
python3 -m pip install -r requirements.txt
```

_Note_: `pip` est un outil permettant d'installer des packages Python.

4. Tout est pr√™t ! Vous pouvez ouvrir le fichier `main.ipynb` et commencer le TP.
   Le fichier `main.ipynb` est un Jupyter Notebook. C'est un outil tr√®s pratique pour faire du d√©veloppement it√©ratif. Il permet d'ex√©cuter du code Python par bloc et de visualiser les r√©sultats directement dans le notebook.

# Donn√©es

Les donn√©es utilis√©es pour notre TP sont issues du jeu de donn√©es publique des [valeurs fonci√®res fran√ßaises de 2021](https://www.data.gouv.fr/en/datasets/demandes-de-valeurs-foncieres/). Ces donn√©es ont √©t√© retravaill√©es pour vous et se trouvent dans le fichier `data.csv`.

# D√©roulement du TP

## Partie 1: entra√Ænement d'un mod√®le de Machine Learning

L'objectif de cette partie est de cr√©er un mod√®le de Machine Learning permettant de pr√©dire le prix d'un bien immobilier √† partir de certaines caract√©ristiques. Une fois ce mod√®le entra√Æn√© nous pourrons le sauvegarder et l'utiliser dans notre application web (partie 2).

Ouvrez le fichier `main.ipynb` et suivez les instructions pour cr√©er votre mod√®le.

## Partie 2: cr√©ation d'une webapp

Afin de d√©velopper le backend de notre application web nous utilisons le [framework Flask](https://flask.palletsprojects.com/en/2.2.x/). L'objectif de cette partie est de comprendre le fonctionnement des serveurs Web et de s'en servir pour appeler notre mod√®le et faire de l'inf√©rence.

1. Lancez le fichier `app.py` avec la commande `python3 app.py`. Votre serveur web va se lancer et √™tre accessible via des requ√™tes HTTP. V√©rifiez que la connexion √† votre serveur fonctionne (Linux/Mac users):

```
curl localhost:5678
```

ou directement depuis votre navigateur http://localhost:5678/hello. Il est possible qu'une URL diff√©rente de `localhost` soit affich√©e dans la console ayant servi √† lancer votre programme, dans ce cas utilisez cette derni√®re.

**Notes**

- `curl` est un outil nous servant ici de client `HTTP`
- Comme vous pouvez le remarquer lorsque l'on utilise la commande `curl`, la donn√©e re√ßue est `<h1>Hello world</h1>`. Lorsque l'on ouvre notre navigateur on peut y voir apparaitre un magnifique **Hello world** format√©. Votre navigateur vous permet de formatter le HTML que vous recevez √† l'√©cran mais ce dernier n'est rien de plus qu'un client HTTP aggr√©ment√© de fonctions d'affichages.

2. Dans le dossier `templates` vous trouverez un fichier _index.html_ qui contient un code HTML un peu plus complexe qu'un simple Hello World. Cr√©er une nouvelle route `/app` permettant de renvoyer √† l'utilisateur le contenu de la page _index.html_. Il est possible d'utiliser la fonction `render_template` de Flask.
   V√©rifier que votre code fonctionne en vous rendant √† l'adresse suivante sur votre navigateur: http://localhost:5678/app.

3. Cr√©er une nouvelle route `/predict` permettant √† l'utilisateur de passer des donn√©es via un [`form`](https://www.w3schools.com/html/html_forms.asp) (requ√™te POST ou GET √† votre avis ?). Cette route devra effectuer dans l'odre:

- **Lecture des donn√©es**: nous transitons l'information de notre client vers notre backend via un formulaire. Flask permet de r√©cup√©rer ces donn√©es dans le corps de la fonction gr√¢ce √† [l'objet request](https://www.digitalocean.com/community/tutorials/processing-incoming-request-data-in-flask). Habituellement les formulaires sont utilis√©s pour les applications Web mais dans le cas de simples APIs il est pr√©f√©rable d'utiliser les formats JSON/XML/Protobuf.
- **V√©rification de la donn√©e**: est-ce que la donn√©e contient tous les champs que l'on souhaite (house_type, nb_room, ...) ? Dans le cas o√π la donn√©e est mal formatt√©e, renvoyer un [code d'erreur 400](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400).
- **Chargement du mod√®le**: apr√®s avoir sauvegard√© votre mod√®le au format `pickle`, il est temps de le recr√©er et d'y faire appel.
- **Transformation des donn√©es**: appliquer le m√™me preprocessing que lors de l'entrainement √† vos nouvelles donn√©es. Vous pouvez directement int√©grer la fonction `transform_house_type` que vous avez cod√© √† la partie pr√©c√©dente.
- **Pr√©diction**: utilisez votre mod√®le charg√© pour pr√©dire le prix √† partir des donn√©es d'entr√©e
- **Renvoi du r√©sultat**: retournez la valeur pr√©dite ainsi que le [status 200](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200).

4. V√©rifiez que votre nouvelle route fonctionne:

```bash
curl -X POST -d "postcode=75&house_type=Maison&house_surface=130&nb_room=5&garden_area=300"  localhost:5678/predict
```

Ou bien testez directement depuis la page web en remplissant le formulaire !

## Partie 3: Int√©grez le code postal √† votre model

Maintenant que vous avez r√©ussi √† d√©ployer votre premi√®re webapp faisant appel √† un mod√®le de Machine Learning, il est temps d'am√©liorer un peu tout √ßa ! L'objectif de cette partie est de rajouter le code postal √† notre mod√®le !

1. R√©cup√©rez des donn√©es permettant d'enrichir votre jeu de donn√©es d'entrainement √† partir des codes postaux. Refaire toute la premi√®re partie en prenant en compte votre/vos nouvelle(s) variable(s).

2. Retravaillez votre backend pour prendre en compte cette nouvelle variable. Il vous faudra revoir votre `preprocessing` pour y ajouter l'enrichissement de donn√©es, et votre √©tape de `validation` des donn√©es. Les donn√©es peuvent √™tre enrichies en ajoutant de nouvelles features, il est possible d'utiliser ce [dataset](https://www.insee.fr/fr/statistiques/4265429?sommaire=4265511) pour ajouter une colonne correspondant au nombre d'habitants par commune.

3. Modifier l'interface web pour ajouter un nouveau champ au formulaire. Un peu de HTML √ßa ne fait pas de mal üòÉ

4. Testez votre nouvelle application et v√©rifiez que les maisons √† Paris co√ªtent tr√®s cher !

## Partie 4: Pour aller plus loin

Cette section vous propose d'explorer diff√©rents sujets auxquels des ing√©nieurs dans le data sont confront√©s. L'objectif est de vous exposer ces probl√©matiques et libre √† vous de creuser les sujets qui vous int√©ressent les plus.

- Aujourd'hui notre application n√©cessite Python et plusieurs d√©pendances pour fonctionner correctement. Dans un contexte de production il est courant de ne pas savoir exactement sur quelle machine tourne son programme (ex: Kubernetes choisi parmi un cluster de machine). Il est impensable de devoir installer toutes les d√©pendances de tous nos programmes sur toutes les machines ! C'est pour r√©pondre √† cette probl√©matique que [Docker](https://www.docker.com/) a √©t√© cr√©√©. Il permet de cr√©er des images qui vont empaqu√™ter toutes les d√©pendances e^t le code n√©cessaire pour faire fonctionner notre application. L'objectif est de cr√©er une image Docker permettant de faire fonctionner notre application web correctement. Vous pouvez vous inspirer de [ce tutoriel](https://www.digitalocean.com/community/tutorials/how-to-build-and-deploy-a-flask-application-using-docker-on-ubuntu-20-04).
- `sklearn` a d√©velopp√© un objet [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) qui permet d'aggr√©ger les √©tapes de preprocessing et de prediction au sein d'un m√™me objet. Il est tout √† fait possible (et m√™me recommand√©) de s√©rializer (avec `pickle` par exemple) le mod√®le accompagn√© de sa pipeline de traitement.
  1. Plut√¥t que d'impl√©menter plusieurs fonctions comme `transform_house_type`, impl√©menter une pipeline de traitement en utilisant les transformeurs de `sklearn`.
  2. Rajoutez une √©tape de standardisation des donn√©es num√©rique. L'objet [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) le fera tr√®s bien pour vous et vous permettra de conserver l'√©cart-type et la variance de votre jeu d'entra√Ænement (qui doit √™tre r√©utilis√©e pour standardiser vos donn√©es de test !!)
  3. S√©rialisez votre pipeline et modifier votre route de pr√©diction pour la simplifier.
- `pickle` est simple d'utilisation mais n'est pas vraiment adapt√© pour des cas d'usage en production. On pr√©f√®rera des formats adapt√©s √† la s√©rialisation de pipeliene de ML comme `Open Neural Network Exchange` (ONNX). L'objectif est de transformer votre code pour exporter votre pipeline sous ce format.
- Dans notre exemple, √† chaque appel √† la route `/predict` on charge le mod√®le et on effectue l'inf√©rence. Dans un exemple tr√®s simple comme le notre cela ne pose pas de probl√®me, mais pour des applications plus gourmandes (ex: Deep Learning), le chargement des mod√®les peut prendre plusieurs minutes. Dans ces cas l√† comment faire ? Quelques id√©es √† explorer:
  - Impl√©mentez notre code en C++
  - `Quantization`: entrainer un r√©seau de neurones sur des float32 au lieu de float64 -> r√©duit la taille du mod√®le
  - `Pruning`: mani√®re de retirer les couches les moins utiles
  - `Distillation`: entra√Æner un r√©seau de neurones plus petit √† r√©pliquer les d√©cisions d'un gros r√©seau
  - Utilisation de GPU pour effectuer l'inf√©rence (pour processer des images ou du texte c'est indispensable)
- Lorsqu'on cherche √† am√©liorer notre mod√®le il est courant d'augmenter la taille de notre jeu de de donn√©es d'entrainement, de rechercher les meilleurs hyperparam√®tres, de repenser l'architecture, ... De fait, on ne peut pas se permettre de repasser par la phase exploratoire dans un Jupyter Notebook, de r√©entrainer notre mod√®le, de le packager et de remplacer le mod√®le existant par le nouveau. En pratique toutes ces √©tapes sont automatis√©es gr√¢ce √† des pipelines sp√©cifiques. Cette automatisation rentre dans le scope du `MLOps` (Machine Learning Operations). On vous invite √† vous renseigner sur ces principes gr√¢ce √† cet excellent site https://ml-ops.org/. Cette discipline est √† cheval entre plusieurs m√©tier: `Data Scientist`, `Data Engineer`, `Software Engineer` et `DevOps`.
